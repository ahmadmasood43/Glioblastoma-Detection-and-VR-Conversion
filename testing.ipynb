{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXpAV0TyPEBVQTIv7A08rP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmadmasood43/Glioblastoma-Detection-and-VR-Conversion/blob/main/testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "zUXPeeruIHq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "qRu9px8yHzw_",
        "outputId": "88fa907b-f32d-4741-d23f-ba20ba7f3082"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.18.2.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np \n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import os\n",
        "import shutil\n",
        "import itertools\n",
        "import imutils\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras import layers\n",
        "from keras.models import Model, Sequential\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "init_notebook_mode(connected=True)\n",
        "RANDOM_SEED = 123"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install tree\n",
        "# create new folders\n",
        "!mkdir TRAIN TEST VAL TRAIN/YES TRAIN/NO TEST/YES TEST/NO VAL/YES VAL/NO\n",
        "!tree -d\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-l23xfYH9-Z",
        "outputId": "fb496313-8f5f-4d51-d66e-367c6d719d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 43.0 kB of archives.\n",
            "After this operation, 115 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 tree amd64 1.8.0-1 [43.0 kB]\n",
            "Fetched 43.0 kB in 0s (159 kB/s)\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 122532 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.8.0-1_amd64.deb ...\n",
            "Unpacking tree (1.8.0-1) ...\n",
            "Setting up tree (1.8.0-1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "\u001b[01;34m.\u001b[00m\n",
            "├── \u001b[01;34mgdrive\u001b[00m\n",
            "│   └── \u001b[01;34mMyDrive\u001b[00m\n",
            "│       ├── \u001b[01;34m2d to 3d images\u001b[00m\n",
            "│       ├── \u001b[01;34mAA\u001b[00m\n",
            "│       ├── \u001b[01;34mannotations\u001b[00m\n",
            "│       ├── \u001b[01;34mAudios from phone 04 2023\u001b[00m\n",
            "│       │   ├── \u001b[01;34mCall recorded\u001b[00m\n",
            "│       │   └── \u001b[01;34mno\u001b[00m\n",
            "│       ├── \u001b[01;34mbrain_test_data\u001b[00m\n",
            "│       ├── \u001b[01;34mbrain_tumor_dataset\u001b[00m\n",
            "│       │   ├── \u001b[01;34mno\u001b[00m\n",
            "│       │   └── \u001b[01;34myes\u001b[00m\n",
            "│       ├── \u001b[01;34mbrain_tumor_dataset_modified\u001b[00m\n",
            "│       │   ├── \u001b[01;34mno\u001b[00m\n",
            "│       │   └── \u001b[01;34myes\u001b[00m\n",
            "│       ├── \u001b[01;34mClassroom\u001b[00m\n",
            "│       │   ├── \u001b[01;34m41-CE-A S&S\u001b[00m\n",
            "│       │   ├── \u001b[01;34mCE 41 class notes A\u001b[00m\n",
            "│       │   ├── \u001b[01;34mComputer Graphics 41 CE\u001b[00m\n",
            "│       │   ├── \u001b[01;34mDATABSE LAB A\u001b[00m\n",
            "│       │   ├── \u001b[01;34mDegree 41\u001b[00m\n",
            "│       │   └── \u001b[01;34mOOP (CE-41-A) DE-41-A\u001b[00m\n",
            "│       ├── \u001b[01;34mColab Notebooks\u001b[00m\n",
            "│       ├── \u001b[01;34mCORT1\u001b[00m\n",
            "│       │   ├── \u001b[01;34mbrain_tumor_dataset\u001b[00m\n",
            "│       │   │   ├── \u001b[01;34mno\u001b[00m\n",
            "│       │   │   └── \u001b[01;34myes\u001b[00m\n",
            "│       │   ├── \u001b[01;34mno\u001b[00m\n",
            "│       │   └── \u001b[01;34myes\u001b[00m\n",
            "│       ├── \u001b[01;34mdatasets_for_drive\u001b[00m\n",
            "│       ├── \u001b[01;34mdatasets_png_for_drive\u001b[00m\n",
            "│       ├── \u001b[01;34mMRIsforfyp\u001b[00m\n",
            "│       ├── \u001b[01;36mMy Drive\u001b[00m -> \u001b[01;34m/content/gdrive/My Drive/\u001b[00m\n",
            "│       ├── \u001b[01;34mnpysegmentedimagesforfyp\u001b[00m\n",
            "│       ├── \u001b[01;34mproject_folder\u001b[00m\n",
            "│       │   ├── \u001b[01;34mlgg-mri-segmentation\u001b[00m\n",
            "│       │   │   └── \u001b[01;34mkaggle_3m\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_CS_4941_19960909\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_CS_4942_19970222\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_CS_4943_20000902\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_CS_4944_20010208\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_CS_5393_19990606\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_CS_5395_19981004\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_CS_5396_20010302\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_CS_5397_20010315\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_CS_6186_20000601\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_CS_6188_20010812\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_CS_6290_20000917\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_CS_6665_20010817\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_CS_6666_20011109\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_CS_6667_20011105\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_CS_6668_20011025\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_CS_6669_20020102\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_5849_19950405\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_5851_19950428\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_5852_19950709\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_5853_19950823\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_5854_19951104\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_5855_19951217\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_5871_19941206\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_5872_19950223\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_5874_19950510\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_6399_19830416\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_6400_19830518\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_6401_19831001\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_6404_19850629\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_6405_19851005\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_6407_19860514\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_6408_19860521\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_7008_19830723\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_7010_19860307\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_7013_19860523\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_7014_19860618\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_7018_19911220\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_7019_19940908\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_7294_19890104\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_7298_19910324\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_7299_19910417\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_7300_19910814\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_7301_19911112\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_7302_19911203\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_7304_19930325\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_7306_19930512\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_7309_19960831\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_8162_19961029\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_8163_19961119\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_8164_19970111\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_8165_19970205\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_8166_19970322\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_8167_19970402\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_8168_19970503\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_A5TP_19970614\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_A5TR_19970726\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_A5TS_19970726\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_A5TT_19980318\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_A5TU_19980312\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_A5TW_19980228\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_DU_A5TY_19970709\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_EZ_7264_20010816\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_FG_5962_20000626\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_FG_5964_20010511\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_FG_6688_20020215\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_FG_6689_20020326\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_FG_6690_20020226\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_FG_6691_20020405\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_FG_6692_20020606\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_FG_7634_20000128\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_FG_7637_20000922\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_FG_7643_20021104\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_FG_8189_20030516\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_FG_A4MT_20020212\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_FG_A4MU_20030903\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_FG_A60K_20040224\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_7473_19970826\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_7475_19970918\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_7602_19951103\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_7605_19950916\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_7608_19940304\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_7616_19940813\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_7680_19970202\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_7684_19950816\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_7686_19950629\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_7690_19960312\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_7692_19960724\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_7693_19950520\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_7694_19950404\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_7855_19951020\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_7856_19950831\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_7860_19960513\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_7874_19950902\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_7877_19980917\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_7879_19981009\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_7881_19981015\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_7882_19970125\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_7884_19980913\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_8018_19970411\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_8105_19980826\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_8106_19970727\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_8107_19980708\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_8111_19980330\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_8113_19930809\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_8114_19981030\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_8563_19981209\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_A5RC_19990831\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_A616_19991226\u001b[00m\n",
            "│       │   │       ├── \u001b[01;34mTCGA_HT_A61A_20000127\u001b[00m\n",
            "│       │   │       └── \u001b[01;34mTCGA_HT_A61B_19991127\u001b[00m\n",
            "│       │   └── \u001b[01;34m__pycache__\u001b[00m\n",
            "│       ├── \u001b[01;34mSE000001\u001b[00m\n",
            "│       ├── \u001b[01;34msegmentedimagesforfyp\u001b[00m\n",
            "│       ├── \u001b[01;34mSegmentedSTLFYP\u001b[00m\n",
            "│       ├── \u001b[01;34mskull brain\u001b[00m\n",
            "│       ├── \u001b[01;34mstlsegmentedimagesforfyp\u001b[00m\n",
            "│       ├── \u001b[01;34mSTLsegmentedimagesjpj\u001b[00m\n",
            "│       ├── \u001b[01;34mWritings\u001b[00m\n",
            "│       └── \u001b[01;34myolov3\u001b[00m\n",
            "├── \u001b[01;34msample_data\u001b[00m\n",
            "├── \u001b[01;34mTEST\u001b[00m\n",
            "│   ├── \u001b[01;34mNO\u001b[00m\n",
            "│   └── \u001b[01;34mYES\u001b[00m\n",
            "├── \u001b[01;34mTRAIN\u001b[00m\n",
            "│   ├── \u001b[01;34mNO\u001b[00m\n",
            "│   └── \u001b[01;34mYES\u001b[00m\n",
            "└── \u001b[01;34mVAL\u001b[00m\n",
            "    ├── \u001b[01;34mNO\u001b[00m\n",
            "    └── \u001b[01;34mYES\u001b[00m\n",
            "\n",
            "166 directories\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_PATH = '/content/gdrive/MyDrive/brain_tumor_dataset/'\n",
        "# split the data by train/val/test\n",
        "for CLASS in os.listdir(IMG_PATH):\n",
        "    if not CLASS.startswith('.'):\n",
        "        IMG_NUM = len(os.listdir(IMG_PATH + CLASS))\n",
        "        for (n, FILE_NAME) in enumerate(os.listdir(IMG_PATH + CLASS)):\n",
        "            img = IMG_PATH + CLASS + '/' + FILE_NAME\n",
        "            if n < 5:\n",
        "                shutil.copy(img, 'TEST/' + CLASS.upper() + '/' + FILE_NAME)\n",
        "            elif n < 0.8*IMG_NUM:\n",
        "                shutil.copy(img, 'TRAIN/'+ CLASS.upper() + '/' + FILE_NAME)\n",
        "            else:\n",
        "                shutil.copy(img, 'VAL/'+ CLASS.upper() + '/' + FILE_NAME)"
      ],
      "metadata": {
        "id": "gWQo6fWUIMjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_data(dir_path, img_size=(100,100)):\n",
        "    \"\"\"\n",
        "    Load resized images as np.arrays to workspace\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "    i = 0\n",
        "    labels = dict()\n",
        "    for path in tqdm(sorted(os.listdir(dir_path))):\n",
        "        if not path.startswith('.'):\n",
        "            labels[i] = path\n",
        "            for file in os.listdir(dir_path + path):\n",
        "                if not file.startswith('.'):\n",
        "                    img = cv2.imread(dir_path + path + '/' + file)\n",
        "                    X.append(img)\n",
        "                    y.append(i)\n",
        "            i += 1\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    print(f'{len(X)} images loaded from {dir_path} directory.')\n",
        "    return X, y, labels\n",
        "\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize = (6,6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    cm = np.round(cm,2)\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "sMd5zr8iI4Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DIR = 'TRAIN/'\n",
        "TEST_DIR = 'TEST/'\n",
        "VAL_DIR = 'VAL/'\n",
        "IMG_SIZE = (224,224)\n",
        "\n",
        "# use predefined function to load the image data into workspace\n",
        "X_train, y_train, labels = load_data(TRAIN_DIR, IMG_SIZE)\n",
        "X_test, y_test, _ = load_data(TEST_DIR, IMG_SIZE)\n",
        "X_val, y_val, _ = load_data(VAL_DIR, IMG_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRbjs6oqIXYG",
        "outputId": "3d146df5-cd61-47db-d9e6-c90cb50f3a30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00,  2.70it/s]\n",
            "<ipython-input-5-59831cb31691>:18: VisibleDeprecationWarning:\n",
            "\n",
            "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "439 images loaded from TRAIN/ directory.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 34.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 images loaded from TEST/ directory.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 28.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "111 images loaded from VAL/ directory.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = dict()\n",
        "y[0] = []\n",
        "y[1] = []\n",
        "for set_name in (y_train, y_val, y_test):\n",
        "    y[0].append(np.sum(set_name == 0))\n",
        "    y[1].append(np.sum(set_name == 1))\n",
        "\n",
        "trace0 = go.Bar(\n",
        "    x=['Train Set', 'Validation Set', 'Test Set'],\n",
        "    y=y[0],\n",
        "    name='No',\n",
        "    marker=dict(color='#33cc33'),\n",
        "    opacity=0.7\n",
        ")\n",
        "trace1 = go.Bar(\n",
        "    x=['Train Set', 'Validation Set', 'Test Set'],\n",
        "    y=y[1],\n",
        "    name='Yes',\n",
        "    marker=dict(color='#ff3300'),\n",
        "    opacity=0.7\n",
        ")\n",
        "data = [trace0, trace1]\n",
        "layout = go.Layout(\n",
        "    title='Count of classes in each set',\n",
        "    xaxis={'title': 'Set'},\n",
        "    yaxis={'title': 'Count'}\n",
        ")\n",
        "fig = go.Figure(data, layout)\n",
        "iplot(fig)"
      ],
      "metadata": {
        "id": "L8LZgOsII-Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_samples(X, y, labels_dict, n=50):\n",
        "    \"\"\"\n",
        "    Creates a gridplot for desired number of images (n) from the specified set\n",
        "    \"\"\"\n",
        "    for index in range(len(labels_dict)):\n",
        "        imgs = X[np.argwhere(y == index)][:n]\n",
        "        j = 10\n",
        "        i = int(n/j)\n",
        "\n",
        "        plt.figure(figsize=(15,6))\n",
        "        c = 1\n",
        "        for img in imgs:\n",
        "            plt.subplot(i,j,c)\n",
        "            plt.imshow(img[0])\n",
        "\n",
        "            plt.xticks([])\n",
        "            plt.yticks([])\n",
        "            c += 1\n",
        "        plt.suptitle('Tumor: {}'.format(labels_dict[index]))\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "JYWj9mW9JFuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_samples(X_train, y_train, labels, 30)\n"
      ],
      "metadata": {
        "id": "jMAw0J8KJKpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RATIO_LIST = []\n",
        "for set in (X_train, X_test, X_val):\n",
        "    for img in set:\n",
        "        RATIO_LIST.append(img.shape[1]/img.shape[0])\n",
        "        \n",
        "plt.hist(RATIO_LIST)\n",
        "plt.title('Distribution of Image Ratios')\n",
        "plt.xlabel('Ratio Value')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8oFlHKCEJNk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_imgs(set_name, add_pixels_value=0):\n",
        "    \"\"\"\n",
        "    Finds the extreme points on the image and crops the rectangular out of them\n",
        "    \"\"\"\n",
        "    set_new = []\n",
        "    for img in set_name:\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "        gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "        # threshold the image, then perform a series of erosions +\n",
        "        # dilations to remove any small regions of noise\n",
        "        thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n",
        "        thresh = cv2.erode(thresh, None, iterations=2)\n",
        "        thresh = cv2.dilate(thresh, None, iterations=2)\n",
        "\n",
        "        # find contours in thresholded image, then grab the largest one\n",
        "        cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        cnts = imutils.grab_contours(cnts)\n",
        "        c = max(cnts, key=cv2.contourArea)\n",
        "\n",
        "        # find the extreme points\n",
        "        extLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
        "        extRight = tuple(c[c[:, :, 0].argmax()][0])\n",
        "        extTop = tuple(c[c[:, :, 1].argmin()][0])\n",
        "        extBot = tuple(c[c[:, :, 1].argmax()][0])\n",
        "\n",
        "        ADD_PIXELS = add_pixels_value\n",
        "        new_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n",
        "        set_new.append(new_img)\n",
        "\n",
        "    return np.array(set_new)"
      ],
      "metadata": {
        "id": "gn6ZDeeqJXBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread('/content/gdrive/MyDrive/brain_tumor_dataset/yes/1 yes.jpg')\n",
        "img = cv2.resize(\n",
        "            img,\n",
        "            dsize=IMG_SIZE,\n",
        "            interpolation=cv2.INTER_CUBIC\n",
        "        )\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "# threshold the image, then perform a series of erosions +\n",
        "# dilations to remove any small regions of noise\n",
        "thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n",
        "thresh = cv2.erode(thresh, None, iterations=2)\n",
        "thresh = cv2.dilate(thresh, None, iterations=2)\n",
        "\n",
        "# find contours in thresholded image, then grab the largest one\n",
        "cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "cnts = imutils.grab_contours(cnts)\n",
        "c = max(cnts, key=cv2.contourArea)\n",
        "\n",
        "# find the extreme points\n",
        "extLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
        "extRight = tuple(c[c[:, :, 0].argmax()][0])\n",
        "extTop = tuple(c[c[:, :, 1].argmin()][0])\n",
        "extBot = tuple(c[c[:, :, 1].argmax()][0])\n",
        "\n",
        "# add contour on the image\n",
        "img_cnt = cv2.drawContours(img.copy(), [c], -1, (0, 255, 255), 4)\n",
        "\n",
        "# add extreme points\n",
        "img_pnt = cv2.circle(img_cnt.copy(), extLeft, 8, (0, 0, 255), -1)\n",
        "img_pnt = cv2.circle(img_pnt, extRight, 8, (0, 255, 0), -1)\n",
        "img_pnt = cv2.circle(img_pnt, extTop, 8, (255, 0, 0), -1)\n",
        "img_pnt = cv2.circle(img_pnt, extBot, 8, (255, 255, 0), -1)\n",
        "\n",
        "# crop\n",
        "ADD_PIXELS = 0\n",
        "new_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n"
      ],
      "metadata": {
        "id": "Usg23LlNJfnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the original image\n",
        "img = cv2.imread('/content/gdrive/MyDrive/brain_tumor_dataset/yes/1 yes.jpg')\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Perform necessary image processing steps to obtain img_cnt, img_pnt, and new_img\n",
        "\n",
        "# Plot the images\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Step 1: Get the original image\n",
        "plt.subplot(141)\n",
        "plt.imshow(img)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.title('Step 1. Get the original image')\n",
        "\n",
        "# Step 2: Find the biggest contour\n",
        "plt.subplot(142)\n",
        "plt.imshow(img_cnt)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.title('Step 2. Find the biggest contour')\n",
        "\n",
        "# Step 3: Find the extreme points\n",
        "plt.subplot(143)\n",
        "plt.imshow(img_pnt)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.title('Step 3. Find the extreme points')\n",
        "\n",
        "# Step 4: Crop the image\n",
        "plt.subplot(144)\n",
        "plt.imshow(new_img)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.title('Step 4. Crop the image')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "b0qYgoUYKAlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_imgs(set_name, add_pixels_value=0):\n",
        "    \"\"\"\n",
        "    Finds the extreme points on the image and crops the rectangular out of them\n",
        "    \"\"\"\n",
        "    set_new = []\n",
        "    for img in set_name:\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "        gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "        # threshold the image, then perform a series of erosions +\n",
        "        # dilations to remove any small regions of noise\n",
        "        thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n",
        "        thresh = cv2.erode(thresh, None, iterations=2)\n",
        "        thresh = cv2.dilate(thresh, None, iterations=2)\n",
        "\n",
        "        # find contours in thresholded image, then grab the largest one\n",
        "        cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        cnts = imutils.grab_contours(cnts)\n",
        "        \n",
        "        if len(cnts) > 0:\n",
        "            c = max(cnts, key=cv2.contourArea)\n",
        "\n",
        "            # find the extreme points\n",
        "            extLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
        "            extRight = tuple(c[c[:, :, 0].argmax()][0])\n",
        "            extTop = tuple(c[c[:, :, 1].argmin()][0])\n",
        "            extBot = tuple(c[c[:, :, 1].argmax()][0])\n",
        "\n",
        "            ADD_PIXELS = add_pixels_value\n",
        "            new_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n",
        "            set_new.append(new_img)\n",
        "\n",
        "    return np.array(set_new)\n",
        "\n",
        "# Apply cropping to each set\n",
        "X_train_crop = crop_imgs(set_name=X_train)\n",
        "X_val_crop = crop_imgs(set_name=X_val)\n",
        "X_test_crop = crop_imgs(set_name=X_test)\n"
      ],
      "metadata": {
        "id": "pWmd5CGKKrD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_new_images(x_set, y_set, folder_name):\n",
        "    i = 0\n",
        "    for (img, imclass) in zip(x_set, y_set):\n",
        "        if imclass == 0:\n",
        "            cv2.imwrite(folder_name+'NO/'+str(i)+'.jpg', img)\n",
        "        else:\n",
        "            cv2.imwrite(folder_name+'YES/'+str(i)+'.jpg', img)\n",
        "        i += 1\n",
        "# saving new images to the folder\n",
        "!mkdir TRAIN_CROP TEST_CROP VAL_CROP TRAIN_CROP/YES TRAIN_CROP/NO TEST_CROP/YES TEST_CROP/NO VAL_CROP/YES VAL_CROP/NO\n",
        "\n",
        "save_new_images(X_train_crop, y_train, folder_name='TRAIN_CROP/')\n",
        "save_new_images(X_val_crop, y_val, folder_name='VAL_CROP/')\n",
        "save_new_images(X_test_crop, y_test, folder_name='TEST_CROP/')"
      ],
      "metadata": {
        "id": "QK-0y5FsM9kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_imgs(set_name, img_size):\n",
        "    \"\"\"\n",
        "    Resize and apply VGG-15 preprocessing\n",
        "    \"\"\"\n",
        "    set_new = []\n",
        "    for img in set_name:\n",
        "        img = cv2.resize(\n",
        "            img,\n",
        "            dsize=img_size,\n",
        "            interpolation=cv2.INTER_CUBIC\n",
        "        )\n",
        "        set_new.append(preprocess_input(img))\n",
        "    return np.array(set_new)\n",
        "X_train_prep = preprocess_imgs(set_name=X_train_crop, img_size=IMG_SIZE)\n",
        "X_test_prep = preprocess_imgs(set_name=X_test_crop, img_size=IMG_SIZE)\n",
        "X_val_prep = preprocess_imgs(set_name=X_val_crop, img_size=IMG_SIZE)"
      ],
      "metadata": {
        "id": "Gho1VTWINFxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set the paramters we want to change randomly\n",
        "demo_datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.05,\n",
        "    height_shift_range=0.05,\n",
        "    rescale=1./255,\n",
        "    shear_range=0.05,\n",
        "    brightness_range=[0.1, 1.5],\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True\n",
        ")"
      ],
      "metadata": {
        "id": "PEXqg3pQNMzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir('preview')\n",
        "x = X_train_crop[0]  \n",
        "x = x.reshape((1,) + x.shape) \n",
        "\n",
        "i = 0\n",
        "for batch in demo_datagen.flow(x, batch_size=1, save_to_dir='preview', save_prefix='aug_img', save_format='jpg'):\n",
        "    i += 1\n",
        "    if i > 20:\n",
        "        break "
      ],
      "metadata": {
        "id": "CSSJj_vuNPar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(X_train_crop[0])\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.title('Original Image')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(15,6))\n",
        "i = 1\n",
        "for img in os.listdir('preview/'):\n",
        "    img = cv2.imread('preview/' + img)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    plt.subplot(3,7,i)\n",
        "    plt.imshow(img)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    i += 1\n",
        "    if i > 3*7:\n",
        "        break\n",
        "plt.suptitle('Augemented Images')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v8GWpWBHNSK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DIR = 'TRAIN_CROP/'\n",
        "VAL_DIR = 'VAL_CROP/'\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    brightness_range=[0.5, 1.5],\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    preprocessing_function=preprocess_input\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input\n",
        ")\n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    TRAIN_DIR,\n",
        "    color_mode='rgb',\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    seed=RANDOM_SEED\n",
        ")\n",
        "\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    VAL_DIR,\n",
        "    color_mode='rgb',\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=16,\n",
        "    class_mode='binary',\n",
        "    seed=RANDOM_SEED\n",
        ")"
      ],
      "metadata": {
        "id": "jw-xPO1fNVsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications import vgg16\n",
        "\n",
        "\n",
        "img_rows, img_cols = 224, 224 \n",
        "\n",
        "\n",
        "vgg = vgg16.VGG16(weights = 'imagenet', \n",
        "                 include_top = False, \n",
        "                 input_shape = (img_rows, img_cols, 3))\n",
        "\n",
        "for layer in vgg.layers[:14]:\n",
        "    layer.trainable = False\n",
        "for layer in vgg.layers[15:17]:\n",
        "    layer.trainable = False    \n",
        "# Let's print our layers \n",
        "for (i,layer) in enumerate(vgg.layers):\n",
        "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
      ],
      "metadata": {
        "id": "36Ch_lN_NhZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lw(bottom_model, num_classes):\n",
        "    \"\"\"creates the top or head of the model that will be \n",
        "    placed ontop of the bottom layers\"\"\"\n",
        "\n",
        "    top_model = bottom_model.output\n",
        "    top_model = GlobalAveragePooling2D()(top_model)\n",
        "    top_model = Dense(1024,activation='relu')(top_model)\n",
        "    top_model = Dense(1024,activation='relu')(top_model)\n",
        "    top_model = Dense(512,activation='relu')(top_model)\n",
        "    top_model = Dense(num_classes,activation='softmax')(top_model)\n",
        "    return top_model "
      ],
      "metadata": {
        "id": "7oXfA4f3Nsrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = 1\n",
        "\n",
        "model = Sequential()\n",
        "model.add(vgg)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(NUM_CLASSES, activation='sigmoid'))\n",
        "\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=RMSprop(lr=1e-4),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "p7nm6070NwXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from tensorflow.keras.models import Model\n",
        "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "MqYqbKhhNzaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5\n",
        "es = EarlyStopping(\n",
        "    monitor='val_acc', \n",
        "    mode='max',\n",
        "    patience=6\n",
        ")\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=50,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=25,\n",
        "    callbacks=[es]\n",
        ")"
      ],
      "metadata": {
        "id": "-YRITgAWPW5_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}